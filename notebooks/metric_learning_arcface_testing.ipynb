{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестирование весов модели, полученных с использованием Metric Learning (ArcFace Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_metric_learning import distances, losses, reducers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "from pytorch_metric_learning.utils.inference import CustomKNN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словарь с метками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = '/Users/chervonikov_alexey/Desktop/VK Video Intern/notebooks/metric_learning/labels/idx2classname.json'\n",
    "\n",
    "with open(dict_path, 'r') as json_file:\n",
    "    idx2classname = json.load(json_file)\n",
    "    \n",
    "# print(idx2classname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополнительные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Базовый transform\n",
    "base_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((300, 300)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "# Денормализация изображения для вывода через plt.imhsow()\n",
    "def unnormalize(tensor, mean=mean, std=std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "def plot_image(image, label):\n",
    "\t'''\n",
    "\tВспомогательная функция для plotting'а тензора torch\n",
    "\t\n",
    "\tПараметры:\n",
    "\t-image: TorchTensor\n",
    "\t-label: str для написания заголовка\n",
    "\t'''\n",
    "\timg = image.squeeze(0).permute(1, 2, 0).numpy()\n",
    "\tfig = plt.figure(figsize = (4, 4))\n",
    "\tplt.imshow(img)\n",
    "\tplt.title(label)\n",
    "\tplt.tight_layout()\n",
    "\tplt.axis('off')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelArcFaceLoss(pl.LightningModule):\n",
    "\n",
    "\t'''\n",
    "\tКласс модели с функцией потерь ArcFaceLoss (наследует методы из pl.LightningModule)\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\tmodel=resnet18(pretrained=True), # базовая модель resnet18\n",
    "\t\t\tembedding_size=128,\n",
    "\t\t\tdistance_metric=distances.CosineSimilarity(),\n",
    "\t\t\treducer=reducers.ThresholdReducer(low=0),\n",
    "\t\t\tloss_fn=losses.ArcFaceLoss,  # Вот она ArcFace из pytorch_metric_learning\n",
    "\t\t\tarcface_margin=0.5,  # margin гиперпараметр\n",
    "\t\t\tarcface_scale=64,  #scale гиперпараметр\n",
    "\t\t\toptimizer=Adam, \n",
    "\t\t\toptimizer_params={'lr': 0.001, 'weight_decay': 0.0001},\n",
    "\t\t\tclass_dict=idx2classname,\n",
    "\t\t\tmin_lr=1e-5,\n",
    "\t\t\tstep_size=8,\n",
    "\t\t\tgamma=0.5\n",
    "\t\t\t):\n",
    "\t\t\n",
    "\t\t'''\n",
    "\t\tКонуструктор объекта класс def __init__(self, ...)\n",
    "\n",
    "\t\tПарамеры:\n",
    "\t\t-model: Базовая модель (default: resnet18(pretrained = True))\n",
    "\t\t-embedding_size: Размер эмбеддингов после сверточных слоев для решения задачи Metric Learning (default = 128)\n",
    "\t\t-distance_metric: Метрика подсчета расстояния между объектами (default: CosineSimilarity())\n",
    "\t\t-reduce: Функция редукции потерь, которая используется для фильтрации значений loss на основе порогового значения.\n",
    "\t\tНапример, ThresholdReducer(low=0) игнорирует все значения потерь ниже 0.\n",
    "\t\tЭто может повысить устойчивость к шуму в данных (default: ThresholdReducer(low=0))\n",
    "\t\t-loss_fn: функция потерь (default: ArcFaceLoss)\n",
    "\t\t-arcface_margin: Смещение угла в формуле функции потерь (default: 0.5)\n",
    "\t\t-arcface_scale: Масшатабирующий параметр в формуле функции потерь (default: 64)\n",
    "\t\t-optimizer: оптимизатор (default: Adam)\n",
    "\t\t-optimizer_params: параметры оптимизатора\n",
    "\t\t-class_dict: словарь Dict label->idx2classname\n",
    "\t\t-min_lr: минимальный шаг сходимости (тот предел, до которого уменьшается lr в процессе обучения)\n",
    "\t\t-step_size: число эпох, через которое экпоненциально уменьшаем шаг сходимости\n",
    "\t\t-gamma: уменьшающий множитель \n",
    "\n",
    "\t\tИнициализирует всё необходимое\n",
    "\n",
    "\t\t'''\n",
    "\n",
    "\t\tsuper(modelArcFaceLoss, self).__init__()\n",
    "\n",
    "\t\t# Модель и её параметры (Архитектура + Функция потерь + Оптимизатор)\n",
    "\t\tself.backbone = model,\n",
    "\t\tself.backbone = self.backbone[0]\n",
    "\t\tself.embedding_size = embedding_size\n",
    "\t\tself.backbone.fc = nn.Linear(self.backbone.fc.in_features, self.embedding_size)\n",
    "\t\tself.fc = nn.Linear(self.embedding_size, self.embedding_size)\n",
    "\t\tself.distance = distance_metric\n",
    "\t\tself.reducer = reducer\n",
    "\t\tself.arcface_margin = arcface_margin\n",
    "\t\tself.arcface_scale = arcface_scale\n",
    "\t\tself.loss_fn = loss_fn(\n",
    "\t\t\tnum_classes=len(class_dict),\n",
    "\t\t\tembedding_size=self.embedding_size,\n",
    "\t\t\tmargin=self.arcface_margin,\n",
    "\t\t\tscale=self.arcface_scale\n",
    "\t\t)\n",
    "\n",
    "\t\tself.optimizer_params = optimizer_params\n",
    "\t\tself.optimizer = optimizer(self.parameters(), **self.optimizer_params)\n",
    "\t\tself.class_dict = class_dict\n",
    "\n",
    "\t\t# Если мы хотим еще параллельно решать задачу классификации на основе привычной CrossEntropy\n",
    "\t\tself.classifier_head = nn.Sequential(\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(in_features=self.embedding_size, out_features=len(self.class_dict))\n",
    "\t\t)\n",
    "\t\tself.classif_loss = torch.nn.CrossEntropyLoss()\n",
    "\t\tself.save_hyperparameters()\n",
    "\t\tself.gamma = gamma\n",
    "\t\tself.step_size = step_size\n",
    "\t\tself.scheduler = StepLR(self.optimizer, step_size=self.step_size, gamma=self.gamma)\n",
    "\t\tself.min_lr = min_lr\n",
    "\n",
    "\t\t# Эмбеддинги для подсчета метрик в конце валидации\n",
    "\t\tself.val_embeddings = []\n",
    "\t\tself.val_labels = []\n",
    "\n",
    "\tdef forward(self, input_x):\n",
    "\t\t'''\n",
    "\t\tforward модели после подачи batch_size:\n",
    "\n",
    "\t\tПараметры:\n",
    "\t\t-self\n",
    "\t\t-input_x: входой пакет картинок\n",
    "\n",
    "\t\tВозвращает эмбеддинг картинки\n",
    "\t\t'''\n",
    "\n",
    "\t\t# Прогон через CNN\n",
    "\t\tcnn_output = self.backbone(input_x)\n",
    "\t\t# Прогон через линейные слои\n",
    "\t\tembedding = self.fc(cnn_output)\n",
    "\t\treturn embedding\n",
    "\n",
    "\tdef training_step(self, batch, batch_idx):\n",
    "\t\t\t'''\n",
    "\t\t\tЧасть train логики: подаем батч, разбиваем на (images, labels)\n",
    "\t\t\tВозвращем loss, по которому будет считаться градиент\n",
    "\t\t\t'''\n",
    "\n",
    "\t\t\timages, labels = batch\n",
    "\t\t\tembeddings = self(images)\n",
    "\n",
    "\t\t\t# ArcFace loss\n",
    "\t\t\tloss_arcface = self.loss_fn(embeddings, labels)\n",
    "\t\t\tfinal_loss = loss_arcface\n",
    "\n",
    "\t\t\tself.log('train_loss', final_loss, sync_dist=True)\n",
    "\t\t\treturn final_loss\n",
    "\n",
    "\tdef on_train_start(self):\n",
    "\t\tself.train()\n",
    "\n",
    "\tdef validation_step(self, batch, batch_idx):\n",
    "\t\t\t\n",
    "\t\t\t'''\n",
    "\t\t\tЛогика на валидации: подаем батч, считаем loss на валидации и записываем в tensor_board\n",
    "\t\t\tИ добавляем эмбеддинги и метки для подсчёта метрик\n",
    "\t\t\t'''\n",
    "\n",
    "\t\t\timages, labels = batch\n",
    "\t\t\tembeddings = self(images)\n",
    "\n",
    "\t\t\tloss_arcface = self.loss_fn(embeddings, labels)\n",
    "\n",
    "\t\t\tfinal_loss = loss_arcface\n",
    "\t\t\tself.log('validation_loss', final_loss, sync_dist=True)\n",
    "\n",
    "\t\t\tself.val_embeddings.append(embeddings)\n",
    "\t\t\tself.val_labels.append(labels)\n",
    "\n",
    "\tdef on_validation_epoch_end(self):\n",
    "\t\t\t\n",
    "\t\t\t'''\n",
    "\t\t\tЛогика в конце валидации: считает ключевую метрики на валидации, а именно precision@1:\n",
    "            \n",
    "\t\t\t-precision@1\n",
    "\t\t\t-Обнуляет массивы эмбеддингов и меток в конце\n",
    "\t\t\t'''\n",
    "\n",
    "\t\t\tall_embeddings = torch.cat(self.val_embeddings)\n",
    "\t\t\tall_labels = torch.cat(self.val_labels)\n",
    "\n",
    "\t\t\taccuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1, knn_func=CustomKNN(\n",
    "\t\t\t\tdistances.CosineSimilarity(), batch_size=64))\n",
    "\n",
    "\t\t\tmetrics = accuracy_calculator.get_accuracy(all_embeddings, all_labels)\n",
    "\t\t\tprecision_at_1 = metrics[\"precision_at_1\"]\n",
    "\t\t\tself.log('precision_at_1_epoch', precision_at_1, sync_dist=True)\n",
    "\n",
    "\t\t\tself.val_embeddings = []\n",
    "\t\t\tself.val_labels = []\n",
    "\n",
    "\tdef on_validation_start(self):\n",
    "\t\t\tself.eval()\n",
    "\n",
    "\tdef configure_optimizers(self):\n",
    "\t\t'''\n",
    "\t\tОбъявление оптимизатора и его фичей\n",
    "\t\t'''\n",
    "\t\t\n",
    "\t\treturn {\n",
    "\t\t\t'optimizer': self.optimizer,\n",
    "\t\t\t'lr_scheduler': {\n",
    "\t\t\t\t'scheduler': self.scheduler,\n",
    "\t\t\t\t'interval': 'epoch',\n",
    "\t\t\t\t'frequency': 1,\n",
    "\t\t\t\t'reduce_on_plateau': False,\n",
    "\t\t\t\t'monitor': 'validation_loss',\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\n",
    "\tdef lr_scheduler_step(self, scheduler, metric):\n",
    "\t\t'''\n",
    "\t\tОбновление шага сходимости\n",
    "\t\t'''\n",
    "\n",
    "\t\tscheduler.step()\n",
    "\t\tself._adjust_learning_rate()\n",
    "\n",
    "\tdef _adjust_learning_rate(self):\n",
    "\t\t'''\n",
    "\t\tПроверка достижения предела learning_rate (self.min_lr)\n",
    "\t\t'''\n",
    "\t\t\n",
    "\t\tfor param_group in self.optimizer.param_groups:\n",
    "\t\t\tparam_group['lr'] = max(param_group['lr'], self.min_lr)\n",
    "\t\t\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо научиться получать эмбеддинги для входных изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.67\n"
     ]
    }
   ],
   "source": [
    "first_asics_path = '/Users/chervonikov_alexey/Desktop/VK Video Intern/notebooks/figures/asics1.jpg'\n",
    "second_asics_path = '/Users/chervonikov_alexey/Desktop/VK Video Intern/notebooks/figures/asics2.jpg'\n",
    "\n",
    "arcface_weights_path = '/Users/chervonikov_alexey/Desktop/VK Video Intern/notebooks/metric_learning/arcface_weights/best-precision-arcfaceloss-epoch=14-precision_at_1_epoch=0.95.ckpt'\n",
    "\n",
    "def make_inference_model_arcface(image_path:str, \n",
    "\t\t\t\t\t\t\t\tmodel_weights_path:str, \n",
    "\t\t\t\t\t\t\t\tmodel):\n",
    "\t'''\n",
    "\tФункция для осуществления инференса\n",
    "\t\n",
    "\tПараметры:\n",
    "\t-image_path: путь к тестируемому изображению (str)\n",
    "\t-model_weights_path: путь к сохраненнымии весами (str)\n",
    "\t-model: класс модели\n",
    "\t'''\n",
    "\n",
    "\timage = Image.open(image_path).convert('RGB')\n",
    "\timage_tensor = base_transform(image)\n",
    "\timage_tensor = image_tensor.unsqueeze(0) \n",
    "\tpl_model = model.load_from_checkpoint(model_weights_path)\n",
    "\tpl_model.eval()\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = pl_model(image_tensor).cpu()[0]\n",
    "\t\n",
    "\treturn output\n",
    "\n",
    "output_first = make_inference_model_arcface(image_path = first_asics_path, \n",
    "\t\t\t\t\t\t\tmodel_weights_path = arcface_weights_path, \n",
    "\t\t\t\t\t\t\tmodel = modelArcFaceLoss)\n",
    "\n",
    "\n",
    "output_second = make_inference_model_arcface(image_path = second_asics_path, \n",
    "\t\t\t\t\t\t\tmodel_weights_path = arcface_weights_path, \n",
    "\t\t\t\t\t\t\tmodel = modelArcFaceLoss)\n",
    "\n",
    "print(f\"Cosine Similarity: {F.cosine_similarity(output_first.unsqueeze(0), output_second.unsqueeze(0))[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Два логотипа *ASICS* имеют близость 0.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: -0.06\n"
     ]
    }
   ],
   "source": [
    "tommy_logo_path = '/Users/chervonikov_alexey/Desktop/VK Video Intern/notebooks/figures/tommy.jpg'\n",
    "output_tommy_logo = make_inference_model_arcface(image_path = tommy_logo_path, \n",
    "\t\t\t\t\t\t\tmodel_weights_path = arcface_weights_path, \n",
    "\t\t\t\t\t\t\tmodel = modelArcFaceLoss)\n",
    "\n",
    "print(f\"Cosine Similarity: {F.cosine_similarity(output_first.unsqueeze(0), output_tommy_logo.unsqueeze(0))[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что разные метрики близости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
